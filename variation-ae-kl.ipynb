{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nimport time\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nmatplotlib.style.use('ggplot')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#arg parse\nap = argparse.ArgumentParser()\nap.add_argument('-e', '--epochs', type=int, default=10,\n    help='number of epochs to train our network for')\nap.add_argument('-l', '--reg_param', type=float, default=0.001, \n    help='regularization parameter `lambda`')\nap.add_argument('-sc', '--add_sparse', type=str, default='yes', \n    help='whether to add sparsity contraint or not')\nargs = vars(ap.parse_args())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting variables\nEPOCHS = 10\nBETA = 0.001\nADD_SPARSITY = 'yes'\nRHO = 0.05\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 32\nprint(f\"Add sparsity regularization: {ADD_SPARSITY}\")","execution_count":5,"outputs":[{"output_type":"stream","text":"Add sparsity regularization: yes\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = datasets.FashionMNIST(\n    root='../output',\n    train=True, \n    download=True,\n    transform=transform\n)\ntestset = datasets.FashionMNIST(\n    root='../output',\n    train=False,\n    download=True,\n    transform=transform\n)\n \n# trainloader\ntrainloader = DataLoader(\n    trainset, \n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n#testloader\ntestloader = DataLoader(\n    testset, \n    batch_size=BATCH_SIZE, \n    shuffle=False\n)","execution_count":16,"outputs":[{"output_type":"stream","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../output/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdbb7d59a738408e815f4bf847057fc2"}},"metadata":{}},{"output_type":"stream","text":"Extracting ../output/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../output/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../output/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23ab7b03e6c4bcdbf19db2a71ffb331"}},"metadata":{}},{"output_type":"stream","text":"Extracting ../output/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../output/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../output/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310fab02159a45818ebc23bff03096b6"}},"metadata":{}},{"output_type":"stream","text":"Extracting ../output/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../output/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../output/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a38b641fba4e089c0c1295ec91d08f"}},"metadata":{}},{"output_type":"stream","text":"Extracting ../output/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../output/FashionMNIST/raw\nProcessing...\nDone!\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401553/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the computation device\ndef get_device():\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n    return device\ndevice = get_device()\n# make the `images` directory\ndef make_dir():\n    image_dir = '../outputs/images'\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\nmake_dir()\n# for saving the reconstructed images\ndef save_decoded_image(img, name):\n    img = img.view(img.size(0), 1, 28, 28)\n    save_image(img, name)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model\nclass SparseAutoencoder(nn.Module):\n    def __init__(self):\n        super(SparseAutoencoder, self).__init__()\n \n        # encoder\n        self.enc1 = nn.Linear(in_features=784, out_features=256)\n        self.enc2 = nn.Linear(in_features=256, out_features=128)\n        self.enc3 = nn.Linear(in_features=128, out_features=64)\n        self.enc4 = nn.Linear(in_features=64, out_features=32)\n        self.enc5 = nn.Linear(in_features=32, out_features=16)\n \n        # decoder \n        self.dec1 = nn.Linear(in_features=16, out_features=32)\n        self.dec2 = nn.Linear(in_features=32, out_features=64)\n        self.dec3 = nn.Linear(in_features=64, out_features=128)\n        self.dec4 = nn.Linear(in_features=128, out_features=256)\n        self.dec5 = nn.Linear(in_features=256, out_features=784)\n \n    def forward(self, x):\n        # encoding\n        x = F.relu(self.enc1(x))\n        x = F.relu(self.enc2(x))\n        x = F.relu(self.enc3(x))\n        x = F.relu(self.enc4(x))\n        x = F.relu(self.enc5(x))\n \n        # decoding\n        x = F.relu(self.dec1(x))\n        x = F.relu(self.dec2(x))\n        x = F.relu(self.dec3(x))\n        x = F.relu(self.dec4(x))\n        x = F.relu(self.dec5(x))\n        return x\nmodel = SparseAutoencoder().to(device)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the loss function\ncriterion = nn.MSELoss()\n# the optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_children = list(model.children())","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kl_divergence(rho, rho_hat):\n    rho_hat = torch.mean(F.sigmoid(rho_hat), 1) # sigmoid because we need the probability distributions\n    rho = torch.tensor([rho] * len(rho_hat)).to(device)\n    return torch.sum(rho * torch.log(rho/rho_hat) + (1 - rho) * torch.log((1 - rho)/(1 - rho_hat)))\n# define the sparse loss function\ndef sparse_loss(rho, images):\n    values = images\n    loss = 0\n    for i in range(len(model_children)):\n        values = model_children[i](values)\n        loss += kl_divergence(rho, values)\n    return loss","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the training function\ndef fit(model, dataloader, epoch):\n    print('Training')\n    model.train()\n    running_loss = 0.0\n    counter = 0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(trainset)/dataloader.batch_size)):\n        counter += 1\n        img, _ = data\n        img = img.to(device)\n        img = img.view(img.size(0), -1)\n        optimizer.zero_grad()\n        outputs = model(img)\n        mse_loss = criterion(outputs, img)\n        if ADD_SPARSITY == 'yes':\n            sparsity = sparse_loss(RHO, img)\n            # add the sparsity penalty\n            loss = mse_loss + BETA * sparsity\n        else:\n            loss = mse_loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / counter\n    print(f\"Train Loss: {epoch_loss:.3f}\")\n    # save the reconstructed images \n    save_decoded_image(outputs.cpu().data, f\"../outputs/images/train{epoch}.png\")\n    return epoch_loss\n# define the validation function\ndef validate(model, dataloader, epoch):\n    print('Validating')\n    model.eval()\n    running_loss = 0.0\n    counter = 0\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(dataloader), total=int(len(testset)/dataloader.batch_size)):\n            counter += 1\n            img, _ = data\n            img = img.to(device)\n            img = img.view(img.size(0), -1)\n            outputs = model(img)\n            loss = criterion(outputs, img)\n            running_loss += loss.item()\n    epoch_loss = running_loss / counter\n    print(f\"Val Loss: {epoch_loss:.3f}\")  \n    # save the reconstructed images \n    outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n    save_image(outputs, f\"../outputs/images/reconstruction{epoch}.png\")\n    return epoch_loss","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\nval_loss = []\nstart = time.time()\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n    train_epoch_loss = fit(model, trainloader, epoch)\n    val_epoch_loss = validate(model, testloader, epoch)\n    train_loss.append(train_epoch_loss)\n    val_loss.append(val_epoch_loss)\nend = time.time()\nprint(f\"{(end-start)/60:.3} minutes\")\ntorch.save(model.state_dict(), f\"../outputs/sparse_ae{EPOCHS}.pth\")","execution_count":28,"outputs":[{"output_type":"stream","text":"  0%|          | 0/1875 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n  0%|          | 1/1875 [00:00<03:53,  8.01it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 1 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:35<00:00, 52.16it/s]\n  7%|▋         | 22/312 [00:00<00:01, 214.10it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.089\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 177.26it/s]                         \n  0%|          | 6/1875 [00:00<00:32, 57.45it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.056\nEpoch 2 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:36<00:00, 51.39it/s]\n  7%|▋         | 22/312 [00:00<00:01, 217.35it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.048\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 174.52it/s]                         \n  0%|          | 5/1875 [00:00<00:41, 45.12it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.039\nEpoch 3 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:36<00:00, 51.94it/s]\n  7%|▋         | 23/312 [00:00<00:01, 222.62it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.043\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 215.17it/s]                         \n  0%|          | 5/1875 [00:00<00:41, 45.04it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.037\nEpoch 4 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:34<00:00, 54.18it/s]\n  7%|▋         | 22/312 [00:00<00:01, 218.95it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.041\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 216.83it/s]                         \n  0%|          | 6/1875 [00:00<00:33, 56.63it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.035\nEpoch 5 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:33<00:00, 56.61it/s]\n  7%|▋         | 23/312 [00:00<00:01, 220.43it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.039\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 216.12it/s]                         \n  0%|          | 5/1875 [00:00<00:39, 47.09it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.033\nEpoch 6 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:34<00:00, 54.48it/s]\n  5%|▌         | 16/312 [00:00<00:01, 156.23it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.038\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 161.97it/s]                         \n  0%|          | 5/1875 [00:00<00:42, 44.02it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.032\nEpoch 7 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:35<00:00, 52.65it/s]\n  5%|▌         | 17/312 [00:00<00:01, 169.02it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.037\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 191.29it/s]                         \n  0%|          | 6/1875 [00:00<00:35, 52.32it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.032\nEpoch 8 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:33<00:00, 56.10it/s]\n  7%|▋         | 22/312 [00:00<00:01, 210.97it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.037\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 211.28it/s]                         \n  0%|          | 6/1875 [00:00<00:33, 55.54it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.031\nEpoch 9 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:35<00:00, 53.04it/s]\n  7%|▋         | 21/312 [00:00<00:01, 207.10it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.036\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 179.35it/s]                         \n  0%|          | 5/1875 [00:00<00:43, 43.41it/s]","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.031\nEpoch 10 of 10\nTraining\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 1875/1875 [00:32<00:00, 57.33it/s]\n  7%|▋         | 22/312 [00:00<00:01, 219.55it/s]","name":"stderr"},{"output_type":"stream","text":"Train Loss: 0.035\nValidating\n","name":"stdout"},{"output_type":"stream","text":"313it [00:01, 218.27it/s]                         ","name":"stderr"},{"output_type":"stream","text":"Val Loss: 0.030\n6.07 minutes\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss plots\nplt.figure(figsize=(10, 7))\nplt.plot(train_loss, color='orange', label='train loss')\nplt.plot(val_loss, color='red', label='validataion loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('../outputs/loss.png')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}